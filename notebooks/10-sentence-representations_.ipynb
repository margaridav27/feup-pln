{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notebook prepared by Henrique Lopes Cardoso (hlc@fe.up.pt).\n",
    "\n",
    "# Sentence Representations\n",
    "\n",
    "In a previous notebook, we have explored different ways of leveraging word embeddings to come up with a representation for an input sequence. Given a sequence of words (tokens), we can get the embeddings for each word in the sequence and either concatenate them, or use some kind of aggregation function such as taking the mean or element-wise max of the embeddings.\n",
    "\n",
    "In the notebook on Transformer models, we have seen how to use a full pretrained BERT-based model as is, or even how to fine-tune the whole model to our task.\n",
    "\n",
    "In this notebook, we explore alternative ways of coming up with richer sentence representations by leveraging language models, while avoiding the need to fine-tune the full BERT-based model. As mentioned in the [BERT paper](https://arxiv.org/abs/1810.04805), a _feature-based approach, where fixed features are extracted from the pretrained model, has certain advantages_. One of them is related with the computational benefits of pre-computing an expensive representation of the data and then running several experiments on top of this representation by resorting to computationally cheaper models.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The dataset and some additional stuff\n",
    "\n",
    "We will be comparing the effect of using different sentence representations for the same text classification task. For that, we start by loading our dataset:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Review</th>\n",
       "      <th>Liked</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Wow... Loved this place.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Crust is not good.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Not tasty and the texture was just nasty.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Stopped by during the late May bank holiday of...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The selection on the menu was great and so wer...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              Review  Liked\n",
       "0                           Wow... Loved this place.      1\n",
       "1                                 Crust is not good.      0\n",
       "2          Not tasty and the texture was just nasty.      0\n",
       "3  Stopped by during the late May bank holiday of...      1\n",
       "4  The selection on the menu was great and so wer...      1"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Importing the dataset\n",
    "dataset = pd.read_csv(\"../data/restaurant_reviews.tsv\", delimiter=\"\\t\", quoting=3)\n",
    "\n",
    "dataset.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For ease of testing different sentence representations, let's define a generic function that given the features used to represent each text entry and the output labels, partitions the dataset into training and testing, trains a (logistic regression) classifier on the training set, and outputs results on the test set.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import (\n",
    "    confusion_matrix,\n",
    "    accuracy_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    f1_score,\n",
    ")\n",
    "\n",
    "\n",
    "def evaluate_feature_representation(X, y):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.20, random_state=0, stratify=y\n",
    "    )\n",
    "\n",
    "    # print(X_train.shape, y_train.shape)\n",
    "    # print(X_test.shape, y_test.shape)\n",
    "\n",
    "    # print(\"\\nLabel distribution in the training set:\")\n",
    "    # print(y_train.value_counts())\n",
    "\n",
    "    # print(\"\\nLabel distribution in the test set:\")\n",
    "    # print(y_test.value_counts())\n",
    "\n",
    "    clf = LogisticRegression()\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "\n",
    "    print(confusion_matrix(y_test, y_pred))\n",
    "    print(\"Accuracy: \", accuracy_score(y_test, y_pred))\n",
    "    print(\"Precision: \", precision_score(y_test, y_pred))\n",
    "    print(\"Recall: \", recall_score(y_test, y_pred))\n",
    "    print(\"F1: \", f1_score(y_test, y_pred))\n",
    "\n",
    "    return"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BERT embeddings\n",
    "\n",
    "We can make use of BERT's internal representation of the input sequence as features. Let's start by loading a BERT model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"distilbert-base-uncased\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_projector.weight', 'vocab_layer_norm.bias', 'vocab_transform.bias', 'vocab_layer_norm.weight', 'vocab_projector.bias', 'vocab_transform.weight']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from transformers import AutoModel\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModel.from_pretrained(model_name)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using the CLS token\n",
    "\n",
    "BERT models add a special CLS token to the beginning of the input sequence. In the model's final hidden state, this token's representation is used as an aggregate sequence representation for classification tasks.\n",
    "\n",
    "Let's see what we get from the representation of the CLS token for\n",
    "a specific example.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wow... Loved this place.\n",
      "tensor([[  101, 10166,  1012,  1012,  1012,  3866,  2023,  2173,  1012,   102]])\n"
     ]
    }
   ],
   "source": [
    "print(dataset[\"Review\"][0])\n",
    "inputs = tokenizer(\n",
    "    dataset[\"Review\"][0], padding=True, truncation=True, return_tensors=\"pt\"\n",
    ")\n",
    "print(inputs[\"input_ids\"])\n",
    "# print(inputs['input_ids'].shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the text has been tokenized to 10 tokens, including the special [CLS] (101) and [SEP] (102) tokens.\n",
    "\n",
    "We now pass the input through BERT and obtain the last hidden state of the model.\n",
    "(Note: if you want to check all hidden states, via _outputs.hidden_states_, you must load the model with the _output_hidden_states=True_ option.)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 10, 768])\n"
     ]
    }
   ],
   "source": [
    "outputs = model(**inputs)\n",
    "# print(outputs.last_hidden_state)   # or outputs[\"last_hidden_state\"]\n",
    "print(outputs.last_hidden_state.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The embeddings size is, in this case, 768, so we have a tensor with dimentions 1x10x768. To get the CLS token embeddings, we access the first one.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([768])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([ 4.2126e-02, -5.0134e-03, -3.5180e-02, -4.1186e-02,  2.2420e-02,\n",
       "        -2.9745e-01,  2.7671e-01,  5.0136e-01, -1.5983e-01, -2.1376e-01,\n",
       "         5.4243e-02, -4.1223e-01, -1.0171e-01,  5.8712e-01,  1.3457e-01,\n",
       "         2.1712e-01, -1.0883e-01,  2.7112e-01,  1.1865e-01, -1.5052e-01,\n",
       "         1.5066e-03, -3.5435e-01, -3.8533e-02,  1.1027e-01, -7.6557e-02,\n",
       "        -3.8043e-02, -1.6030e-02, -2.0002e-01,  1.3438e-01,  2.5308e-02,\n",
       "         1.4056e-02,  8.4898e-02, -2.6438e-01, -1.7116e-01, -1.0666e-01,\n",
       "        -9.9342e-02, -1.1813e-02, -1.0511e-01, -2.2373e-01,  1.2219e-01,\n",
       "        -3.2356e-02, -1.3577e-01,  1.8531e-01, -1.0054e-01, -1.7740e-01,\n",
       "        -3.3294e-01, -2.3977e+00,  5.2049e-02, -2.2174e-02, -1.1433e-01,\n",
       "         4.4919e-01,  8.9583e-03,  1.7044e-01,  2.8112e-01,  3.2639e-01,\n",
       "         4.3146e-01, -2.8787e-01,  4.8212e-01,  1.5943e-02, -7.4780e-02,\n",
       "         3.7156e-01,  3.0133e-02, -1.2007e-01, -1.6729e-01, -6.9575e-02,\n",
       "         2.2837e-01,  5.0202e-02,  5.9328e-02, -4.7378e-02,  4.0569e-01,\n",
       "        -1.4405e-01, -2.3732e-01,  2.1693e-01,  1.5723e-02,  2.8111e-02,\n",
       "         1.4401e-03, -3.6878e-02,  1.5349e-01, -2.0966e-02,  1.7578e-01,\n",
       "        -9.0157e-02,  3.8237e-01,  2.7625e-01,  2.2457e-01,  8.4042e-02,\n",
       "         3.0411e-01, -1.5820e-01, -2.0072e-01,  2.3321e-01,  3.5148e-01,\n",
       "        -1.7013e-01, -1.5750e-01,  2.0314e-01,  1.4751e-01,  3.4894e-01,\n",
       "        -2.3931e-01,  8.5056e-02,  1.4879e-02,  1.3871e-01,  2.8587e-01,\n",
       "         1.7711e-01, -1.5566e-01,  1.7220e-02, -3.7086e-01,  5.9842e-03,\n",
       "        -9.5596e-02,  1.2363e-01, -4.2338e-01,  7.0658e-02, -2.3925e+00,\n",
       "         2.2370e-01,  1.2969e-01, -1.4900e-01, -4.1786e-01, -8.7277e-02,\n",
       "         4.7200e-01,  2.6129e-01,  6.4794e-02,  1.3629e-01,  1.0015e-01,\n",
       "        -2.0224e-01,  3.9864e-01,  1.8120e-02, -1.1659e-01, -8.0913e-02,\n",
       "         2.1758e-01,  3.7341e-04, -1.4869e-01, -9.1727e-02,  2.4433e-01,\n",
       "         4.2877e-01,  4.8204e-01,  4.7300e-02, -6.8018e-02, -2.2574e-01,\n",
       "         7.1878e-03,  1.3851e-01, -2.0540e-01, -3.2487e-01,  1.6443e-02,\n",
       "        -2.8310e-01,  2.4807e-02, -2.9069e+00,  3.8468e-01,  1.3711e-01,\n",
       "        -3.3785e-02,  1.8423e-02,  4.2002e-02, -6.9649e-02,  2.6117e-01,\n",
       "         3.4192e-01,  6.2952e-02, -2.0202e-01, -1.2359e-01, -2.3382e-01,\n",
       "         1.0151e-01, -3.1984e-01, -1.0126e-02,  3.2201e-01,  2.0493e-01,\n",
       "         1.3229e-01, -2.1264e-02, -1.8377e-01,  4.5356e-03, -6.4141e-02,\n",
       "         2.5781e-01,  2.6947e-01,  2.4629e-01,  4.4860e-03, -7.7868e-02,\n",
       "        -1.4473e-01,  9.3573e-02,  1.7084e-01, -1.6523e-01, -3.8921e-02,\n",
       "        -4.5907e-02,  2.5705e-01,  3.4598e-01,  4.0850e-02, -6.2060e-02,\n",
       "        -8.8856e-02,  4.4058e-01,  9.4303e-02,  1.2055e-01,  1.4983e-01,\n",
       "        -9.4369e-02,  3.5480e-01, -1.7392e-01, -2.4470e-01,  2.1054e-01,\n",
       "        -1.7208e-01, -6.9006e-02, -1.2508e-01, -1.2717e-01,  3.6729e-01,\n",
       "        -6.0591e-02, -6.1474e-02, -3.0325e-01,  3.4305e-01,  7.1559e-02,\n",
       "        -2.0832e-01,  3.5274e-02,  1.4219e-01,  4.7871e-02, -1.3664e-01,\n",
       "         3.6173e+00,  7.5486e-02,  6.8678e-02,  2.2100e-01,  1.1923e-01,\n",
       "        -1.4159e-01,  3.7123e-02,  1.4604e-02, -1.1898e-01, -2.4922e-02,\n",
       "         4.1181e-02,  2.5487e-01,  7.7492e-02, -4.0592e-02, -2.8885e-01,\n",
       "         2.2542e-01,  1.5546e-01, -3.0844e-01,  2.1736e-01, -5.0748e-02,\n",
       "         1.4545e-01,  1.8265e-01, -3.8623e-03, -2.2005e-02, -1.3640e+00,\n",
       "        -1.1928e-01, -1.6184e-01, -8.0701e-02,  3.4085e-01, -1.4698e-01,\n",
       "         8.6008e-02,  2.0831e-02,  3.1537e-03, -1.2470e-01,  8.2941e-02,\n",
       "         6.1560e-02, -9.5327e-03,  4.6648e-01,  2.5271e-01, -2.9918e-01,\n",
       "         4.7592e-01,  2.4147e-01, -8.4807e-02, -5.5121e-02, -1.4060e-01,\n",
       "         2.2615e-01,  1.0029e-01,  8.6911e-02, -9.6794e-02,  2.5183e-01,\n",
       "        -4.7121e-02,  1.7460e-02, -6.6585e-02, -3.4809e-01, -1.6077e-01,\n",
       "        -2.3914e-01, -1.7408e-01,  1.8941e-01,  4.2553e-02, -3.4635e-01,\n",
       "        -1.5251e-01,  2.6493e-01, -2.0997e-01,  1.0548e-01,  1.5727e-01,\n",
       "        -1.3675e-01,  7.0187e-02, -1.9028e-01, -3.6550e+00,  1.4351e-01,\n",
       "        -1.3790e-01,  1.8184e-01,  3.4438e-01,  8.2981e-02,  1.3643e-01,\n",
       "         1.0064e-01,  4.2770e-01, -4.7877e-01,  2.5493e-01,  1.9207e-01,\n",
       "        -1.9739e-01,  1.2150e-01, -2.4600e-01,  2.3331e-01, -1.1693e-01,\n",
       "         7.5280e-02, -2.3049e-01, -1.5410e-01, -2.5738e-02,  2.3922e-01,\n",
       "        -1.8760e-01,  3.4283e-01, -4.4325e-02, -1.4340e-01,  1.0097e-01,\n",
       "        -2.6415e-01,  1.7407e-01, -6.3359e-02, -9.5248e-02, -1.0000e-01,\n",
       "         1.7822e-01, -1.2345e-01, -2.5736e-01, -2.5787e+00, -8.5157e-02,\n",
       "        -4.6022e-02, -2.6841e-01, -1.8827e-01,  1.7832e-02,  4.2554e-01,\n",
       "        -2.4044e-01, -3.0354e-01,  3.4488e-02,  3.1526e-01, -1.0206e-01,\n",
       "        -1.5256e-03,  3.6450e-01,  4.1592e-01, -5.1963e-02,  2.5402e-01,\n",
       "        -8.2877e-02,  1.4699e-02,  6.2597e-02, -3.0990e-03, -7.1008e-02,\n",
       "        -1.6699e-01, -1.1351e-01,  2.0812e-01,  3.9101e-01, -5.6791e-01,\n",
       "         7.8680e-02, -3.0719e-01,  1.0881e-01, -4.9699e-02, -1.5255e-01,\n",
       "         1.0030e-01,  4.0624e-02, -2.5070e-01, -1.6749e-01,  1.4669e-01,\n",
       "         1.0953e-01,  3.4210e-01, -9.2850e-02, -6.0615e-02,  4.9192e-01,\n",
       "         1.0765e-02,  1.3101e-01,  5.9579e-01,  4.9438e-02,  3.2312e-01,\n",
       "        -1.2724e-01, -2.2387e-01,  1.4488e-01, -3.4761e-02,  1.4972e-01,\n",
       "         1.3164e+00,  4.3778e-03,  4.6541e-02, -3.7200e-01,  4.3305e-01,\n",
       "         2.4575e-01, -4.4244e-02,  4.4967e-02,  4.2494e-01, -1.8549e-01,\n",
       "         2.3430e-01, -1.0653e-01, -6.1295e-02, -1.5101e-01,  1.6356e-01,\n",
       "        -3.4057e-01, -1.6820e-01,  1.9364e-01,  2.0661e-02,  3.6975e-03,\n",
       "        -2.1094e-02, -9.2379e-01, -1.9230e-01,  1.4827e-01, -3.2228e-02,\n",
       "         4.9120e-02, -1.0064e-01,  2.1683e-01, -2.0265e-01, -1.6510e-01,\n",
       "        -4.9570e-02,  3.9284e-01, -2.1703e-01, -9.1303e-03, -2.4749e-01,\n",
       "        -1.1771e-02, -4.1914e-01,  4.8533e-02, -1.5484e-01,  1.3600e-01,\n",
       "        -1.9985e-02,  1.1264e-01,  9.5540e-02, -1.0526e-01,  3.6483e-01,\n",
       "        -7.0042e-01,  3.0988e-01, -3.0270e-01, -1.0616e-01, -2.6374e-01,\n",
       "        -1.8652e-01,  4.4183e-01, -1.9821e-01, -1.7725e-01, -2.3332e-01,\n",
       "         2.3286e-01, -1.7865e-01,  1.1710e-01, -1.7805e-02,  2.7736e-02,\n",
       "         5.7024e-02,  1.6409e-01,  9.6983e-01, -1.9023e-01,  5.7272e-02,\n",
       "         3.4079e-01, -8.7467e-02,  2.8122e-01,  3.0145e-01,  2.0024e-01,\n",
       "        -3.4867e-01,  1.7990e-01,  9.1056e-02, -5.4801e-02,  6.8528e-02,\n",
       "        -1.5858e-01, -4.0623e-01, -8.1280e-02,  2.1087e-01, -1.0023e-02,\n",
       "        -1.4946e-01, -3.4760e-01, -5.3835e-02, -1.1329e-01, -1.5312e-01,\n",
       "        -9.7466e-02,  3.4323e-01,  1.2917e-01,  3.0988e-01,  2.5553e-01,\n",
       "        -2.0630e-01,  6.9401e-01, -8.8239e-02,  5.8322e-01, -9.2880e-02,\n",
       "         1.0082e-01, -1.9130e-01,  9.1433e-02, -1.2754e-01, -1.8011e-01,\n",
       "        -8.2964e-04, -1.1882e-01,  1.4392e-01, -2.4378e-02,  3.9703e-02,\n",
       "         1.3104e-02, -5.7194e-02, -5.1831e-02,  1.8084e-01,  8.9595e-02,\n",
       "        -1.2843e+00,  4.7253e-01,  1.0080e-01,  1.0627e-01,  3.8408e-02,\n",
       "         5.4381e-02, -6.0893e-02,  2.8475e-01,  4.2534e-01,  1.5533e-01,\n",
       "        -1.3061e-01,  2.9634e-02, -3.0470e-01,  8.9034e-02, -1.2709e-01,\n",
       "        -1.3241e-01, -4.5312e-02, -5.2188e-02,  5.1149e-02, -7.1532e-03,\n",
       "         1.4968e-01,  1.4819e-01,  1.5612e-01,  3.1696e-02, -1.9417e-01,\n",
       "        -9.7441e-02,  1.0080e-04,  3.3750e-01,  1.8574e-01, -8.0506e-02,\n",
       "        -1.2884e-01, -3.2765e-01, -6.5778e-01, -3.3726e-01,  3.6457e-01,\n",
       "        -2.3588e-01, -1.5541e-01,  3.9425e-02,  1.4262e-01, -1.0727e-01,\n",
       "        -4.1359e-01,  2.7814e-01,  1.0414e-02, -2.4378e-01,  6.4356e-01,\n",
       "         2.6001e-01, -2.9927e-02,  1.5113e-01,  5.2135e-02, -1.5699e-01,\n",
       "         3.3673e-03, -1.2261e-01, -1.3204e-02,  2.9347e-02,  2.6364e-02,\n",
       "        -3.0074e-02, -5.2011e-02,  2.5618e-02, -2.7385e-01,  3.8316e-02,\n",
       "         3.0065e-01, -3.6896e-01, -2.6824e-01,  1.5647e-01, -2.6901e-01,\n",
       "        -6.5518e-01, -3.3510e-01, -3.4635e-01, -1.3826e-01,  2.3680e-02,\n",
       "         4.0250e-01,  1.0770e-02, -2.8306e-01,  5.7154e-02, -1.7183e-01,\n",
       "         9.6556e-02,  2.4602e-02,  1.1394e-01,  2.4604e-01, -3.3882e-01,\n",
       "         1.2940e-01, -1.9465e-01, -3.2074e-01,  2.0405e-01, -1.7469e-01,\n",
       "         1.5722e-01, -5.8411e-02,  4.8077e-02, -1.0870e-01, -1.2751e-01,\n",
       "        -3.2090e-01, -2.3129e-01,  1.0009e-01,  2.8446e-02,  6.7419e-02,\n",
       "         3.1242e-01,  2.8553e-01, -2.2842e-01, -1.5457e-01, -2.1178e-02,\n",
       "        -1.3282e-01,  3.9061e-01,  3.4300e-01,  1.8449e-01,  2.2007e-01,\n",
       "         2.0610e-01,  2.7937e-01,  9.0339e-04, -1.6447e-01,  1.1032e-01,\n",
       "        -1.7722e-01,  2.7843e-02, -7.3328e-02, -9.2759e-02,  7.1998e-03,\n",
       "        -4.1248e-01,  9.2161e-02, -3.1397e-01,  1.9149e+00,  4.0297e-01,\n",
       "         2.5977e-01, -1.9786e-02,  2.1891e-01, -3.0021e-01, -1.5752e-01,\n",
       "         5.3642e-02, -9.6213e-03,  4.1968e-01, -2.1234e-01,  9.8226e-02,\n",
       "        -7.6389e-02,  2.2943e-01,  5.2906e-01,  2.0905e-01,  8.6041e-03,\n",
       "        -1.3575e-01, -5.0100e-01,  1.3436e-02, -2.8201e-01,  2.1731e-01,\n",
       "         2.5680e-01,  4.4948e-02, -2.8156e-01,  3.7578e-01,  2.1678e-01,\n",
       "        -4.8019e-02,  1.2057e-01,  3.3510e-01, -1.3319e-01, -1.6017e-01,\n",
       "        -6.0983e-02,  2.8816e-01, -2.6104e-01, -1.4091e-01,  5.4814e-02,\n",
       "        -1.7709e-01, -8.9378e-02, -6.6977e-02,  1.0386e-01, -2.0366e-01,\n",
       "         4.0666e-01, -4.9152e-03, -7.2162e-02,  5.4576e-01, -1.6748e-01,\n",
       "        -2.4263e-01,  2.0807e-01,  4.6379e-01, -9.0587e-02, -1.3582e-02,\n",
       "        -4.1327e-01,  1.6876e-01, -6.2771e-02, -3.2571e-01, -1.9763e-01,\n",
       "        -3.0850e-02,  1.9075e-02,  2.9012e-02,  4.0602e-02,  1.6030e-01,\n",
       "         2.7572e-01,  3.0042e-01,  2.4874e-01,  8.9777e-02, -2.9654e-01,\n",
       "         5.0770e-02,  1.1381e-01, -2.9499e-01, -1.2717e-01,  2.5338e-01,\n",
       "         3.5330e-01, -7.3474e-02,  2.4344e-01,  5.4995e-02,  4.5807e-01,\n",
       "         7.8534e-02, -1.4707e-01, -2.6021e+00,  3.0406e-01,  6.3383e-02,\n",
       "         9.9005e-02, -6.2809e-04,  4.9088e-01,  4.1428e-01, -2.2562e-01,\n",
       "         1.4252e-01, -1.8755e-01,  1.1274e-01,  2.7347e-01,  3.4898e-01,\n",
       "         1.1508e-01,  1.3750e-01, -7.4340e-02,  3.6953e-01, -6.3773e-02,\n",
       "        -1.5666e-01, -1.4660e-01, -1.1169e-01,  2.5078e-01,  1.0482e-02,\n",
       "        -4.1351e-01, -4.4121e-01,  3.0685e-01, -1.2988e-01, -1.3937e-01,\n",
       "         1.4553e-01,  3.2262e-02, -1.1523e-01,  4.0294e-01, -2.7716e-01,\n",
       "        -9.0148e-02,  1.1433e-01, -1.1847e-01, -2.7636e-01, -2.0759e-02,\n",
       "         6.8683e-02,  4.7679e-02, -1.9596e-02,  4.8041e-01, -4.9534e-02,\n",
       "         1.7207e-01, -3.0618e-01, -1.5203e-01,  3.5455e-01, -1.2384e-01,\n",
       "         9.1036e-02, -3.8459e-01, -1.5136e-01,  7.2883e-02,  4.0531e-01,\n",
       "         7.1884e-02,  2.3774e-01,  4.8482e-02,  6.5139e-02, -9.1425e-03,\n",
       "         1.3411e-01, -1.1159e-01, -3.6600e-02, -4.7007e-02, -4.0248e-02,\n",
       "        -3.6267e-02,  4.4337e-01, -7.6300e-02, -1.7624e-01,  1.2577e-02,\n",
       "        -1.6020e-06, -1.2568e-01, -1.7031e-01, -7.0217e-02,  1.0897e-01,\n",
       "         7.5584e-02,  1.4898e-01, -3.3421e-02,  3.5118e-01,  1.6286e-01,\n",
       "         6.4630e-03,  2.2194e-02,  2.9539e-02, -1.9952e-01, -3.0854e-02,\n",
       "        -1.5751e-01,  2.4500e-02, -7.5583e+00, -1.8171e-01, -1.8991e-01,\n",
       "        -3.1234e-01,  1.3375e-02, -1.8852e-01,  1.0263e-01, -3.0383e-01,\n",
       "         2.0007e-01, -3.1152e-01,  1.6558e-01,  3.8005e-02, -2.4550e-01,\n",
       "        -1.4343e-01,  3.2029e-01,  1.7422e-02], grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(outputs.last_hidden_state[0][0].shape)\n",
    "outputs.last_hidden_state[0][0]  # the CLS token is the first one"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can get the CLS token embeddings for every review. For that, we need to convert each tensor object into a numpy.ndarray by using the _numpy()_ method.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "X = np.empty([0, 768])\n",
    "X = np.array(\n",
    "    [\n",
    "        model(**tokenizer(rev, padding=True, truncation=True, return_tensors=\"pt\"))\n",
    "        .last_hidden_state[0][0]\n",
    "        .detach()\n",
    "        .numpy()\n",
    "        for rev in dataset[\"Review\"]\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We get the labels and check the shape of the feature matrix. Each input element should have 768 features (the dimension of encoder layers in Distill BERT, aka the hidden size for the BERT base model).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 768) (1000,)\n"
     ]
    }
   ],
   "source": [
    "y = dataset[\"Liked\"]\n",
    "print(X.shape, y.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how this representation fares with our generic evaluation function, which trains and tests a classifier based on the representation we provide to it.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[88 12]\n",
      " [ 9 91]]\n",
      "Accuracy:  0.895\n",
      "Precision:  0.883495145631068\n",
      "Recall:  0.91\n",
      "F1:  0.896551724137931\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/margarida/.local/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "evaluate_feature_representation(X, y)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Averaging over token embeddings\n",
    "\n",
    "Alternatively, we can also average across the embeddings for all tokens in the last hidden state. In fact, even though the [BERT](https://arxiv.org/abs/1810.04805) paper suggests the CLS token be used as a representation of the input sequence for classification tasks, in some cases averaging across embeddings obtains improved performance. Can you try it out?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 768) (1000,)\n",
      "[[90 10]\n",
      " [ 9 91]]\n",
      "Accuracy:  0.905\n",
      "Precision:  0.900990099009901\n",
      "Recall:  0.91\n",
      "F1:  0.9054726368159204\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/margarida/.local/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "# your code here\n",
    "X = np.empty([0, 768])\n",
    "y = dataset[\"Liked\"]\n",
    "\n",
    "X = np.array(\n",
    "    [\n",
    "        model(**tokenizer(rev, padding=True, truncation=True, return_tensors=\"pt\"))\n",
    "        .last_hidden_state[0]\n",
    "        .mean(axis=0)\n",
    "        .detach()\n",
    "        .numpy()\n",
    "        for rev in dataset[\"Review\"]\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(X.shape, y.shape)\n",
    "evaluate_feature_representation(X, y)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SBERT (SentenceTransformers)\n",
    "\n",
    "Several other sentence representation models exist, and we here explore the usage of [SentenceTransformers](https://www.sbert.net/). Although this framework has been built having semantic similarity tasks in mind, these representations can also be used for text classification tasks, as evidenced in the [original paper](https://arxiv.org/abs/1908.10084).\n",
    "\n",
    "SBERT uses a modification of the BERT network using a siamese architecture and a triplet loss function, trained with Natural Language Inference data ([SNLI](https://nlp.stanford.edu/projects/snli/)).\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing BERT and SBERT representations\n",
    "\n",
    "To compare the representations obtained by BERT and those provided by SentenceTransformers, we can see how similar those representations are for a few sentences.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [\n",
    "    \"A man is eating food.\",\n",
    "    \"A man is eating a piece of bread.\",\n",
    "    \"The quick brown fox jumps over the lazy dog.\",\n",
    "]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start with BERT, while making use of an util facility provided by SentenceTransformers to compute cosine similarities.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000, 0.9867, 0.9358],\n",
       "        [0.9867, 1.0000, 0.9248],\n",
       "        [0.9358, 0.9248, 1.0000]])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sentence_transformers import util\n",
    "\n",
    "embeddings = np.empty([0, 768])\n",
    "embeddings = np.array(\n",
    "    [\n",
    "        model(**tokenizer(s, padding=True, truncation=True, return_tensors=\"pt\"))\n",
    "        .last_hidden_state[0][0]\n",
    "        .detach()\n",
    "        .numpy()\n",
    "        for s in sentences\n",
    "    ]\n",
    ")\n",
    "\n",
    "cos_sim = util.cos_sim(embeddings, embeddings)\n",
    "cos_sim"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, all sentence representation pairs have very high cosine similarities.\n",
    "This can be somewhat alleviated by averaging across the embeddings for all tokens in the last hidden state, but the sentences will still have an unexpectedly high cosine similarity.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now load a SentenceTransformer model and see what it gives us.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "sbert_model = SentenceTransformer(\"all-MiniLM-L6-v2\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using SentenceTransformers consists of simply encoding the sentences that we have, in a single step.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.0000,  0.7553, -0.0220],\n",
       "        [ 0.7553,  1.0000,  0.0033],\n",
       "        [-0.0220,  0.0033,  1.0000]])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sbert_embeddings = sbert_model.encode(sentences)\n",
    "\n",
    "cos_sim = util.cos_sim(sbert_embeddings, sbert_embeddings)\n",
    "cos_sim\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using SBERT embeddings for classification\n",
    "\n",
    "We now use SentenceTransformer embeddings for our classification problem. For that, we need to encode the reviews in the dataset. You will find that this step is much faster than doing it using BERT.\n",
    "Then, we can use our generic function to train and test a classifier by passing it the reviews' embeddings and the labels.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[90 10]\n",
      " [12 88]]\n",
      "Accuracy:  0.89\n",
      "Precision:  0.8979591836734694\n",
      "Recall:  0.88\n",
      "F1:  0.888888888888889\n"
     ]
    }
   ],
   "source": [
    "# your code here\n",
    "X = sbert_model.encode(dataset[\"Review\"])\n",
    "y = dataset[\"Liked\"]\n",
    "evaluate_feature_representation(X, y)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SimCSE: Simple Contrastive Learning of Sentence Embeddings\n",
    "\n",
    "[SimCSE](https://github.com/princeton-nlp/SimCSE) is another recent model that trains a BERT-based model using contrastive learning.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's load a SimCSE model and see what it gives us as sentence representations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from simcse import SimCSE\n",
    "\n",
    "simcse_model = SimCSE(\"princeton-nlp/sup-simcse-bert-base-uncased\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can easily obtain sentence embeddings:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simcse_model.encode(sentences)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But SimCSE's API allows us to obtain similarity scores directly from the source sentences.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simcse_model.similarity(sentences, sentences)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare these with those obtained using SBERT.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using SimCSE embeddings for classification\n",
    "\n",
    "We now use SimCSE embeddings for our classification problem.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
