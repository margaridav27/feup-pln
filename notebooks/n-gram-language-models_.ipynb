{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "1GaX95nnkQbu"
   },
   "source": [
    "Notebook prepared by Henrique Lopes Cardoso (hlc@fe.up.pt), based on [A Comprehensive Guide to Build your own Language Model in Python](https://www.analyticsvidhya.com/blog/2019/08/comprehensive-guide-language-model-nlp-python-code/) by Mohd Sanad Zaki Rizvi.\n",
    "\n",
    "# N-GRAM LANGUAGE MODELS\n",
    "\n",
    "N-gram language models are based on computing probabilities for the occurrence of each word given _n-1_ previous words.\n",
    "\n",
    "To \"train\" such models, we will make use of the [Reuters](https://www.nltk.org/book/ch02.html) corpus, which contains 10,788 news documents in a total of 1.3 million words.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 2129,
     "status": "ok",
     "timestamp": 1645451765839,
     "user": {
      "displayName": "Henrique Lopes Cardoso",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiDm6eMLxxGrmSt7hX7Fe3fSYQXSU2koR-YesCvdw=s64",
      "userId": "16701394035750291027"
     },
     "user_tz": 0
    },
    "id": "HqoClhOHkQb2"
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import reuters\n",
    "\n",
    "nltk.download(\"reuters\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can check the number of sentences there are in the corpus. Each sentence is a list of words.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(reuters.sents()))\n",
    "\n",
    "print(reuters.sents()[0])\n",
    "for w in reuters.sents()[0]:\n",
    "    print(w, end=\" \")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "xSbkDouwkQb4"
   },
   "source": [
    "## Unigram model\n",
    "\n",
    "For starters, let's build a unigram language model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 25,
     "status": "error",
     "timestamp": 1645451769412,
     "user": {
      "displayName": "Henrique Lopes Cardoso",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiDm6eMLxxGrmSt7hX7Fe3fSYQXSU2koR-YesCvdw=s64",
      "userId": "16701394035750291027"
     },
     "user_tz": 0
    },
    "id": "VaLWQjxmkQb5",
    "outputId": "67ec3ca8-4982-46c7-a1a2-3cba411b8956"
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "# Create a placeholder for the model\n",
    "uni_model = defaultdict(int)\n",
    "\n",
    "# Count the frequency of each token\n",
    "for sentence in reuters.sents():\n",
    "    for w in sentence:\n",
    "        uni_model[w] += 1\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the counts, we need to transform them into probabilities:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_count = float(sum(uni_model.values()))\n",
    "for w in uni_model:\n",
    "    uni_model[w] /= total_count\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uni_model\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Likely words\n",
    "\n",
    "How likely is the word 'the'?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n",
    "print(uni_model[\"the\"])\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is the most likely word in the corpus?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n",
    "print(max(uni_model, key=uni_model.get))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generating text\n",
    "\n",
    "Based on this unigram language model, we can try generating some text. It will not be pretty, though...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-IsAHfWikQb6"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "# number of words to generate\n",
    "total_words = 100\n",
    "text = []\n",
    "\n",
    "for i in range(total_words):\n",
    "    # select a random probability threshold\n",
    "    r = random.random()\n",
    "\n",
    "    # select word above the probability threshold\n",
    "    accumulator = 0.0\n",
    "    for word in uni_model.keys():\n",
    "        accumulator += uni_model[word]\n",
    "        if accumulator >= r:\n",
    "            text.append(word)\n",
    "            break\n",
    "\n",
    "print(\" \".join([t for t in text]))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "iNyUfDqNkQb7"
   },
   "source": [
    "## Bigram model\n",
    "\n",
    "In a bigram model, we'll compute the probability of each word given the previous word as context. To obtain bigrams, we can use NLTK's [bigrams](https://www.nltk.org/_modules/nltk/util.html#bigrams). When doing so, we can padd the input left and right and define our own sequence start and sequence end symbols.\n",
    "\n",
    "We first need to obtain the counts:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HOIR6wVBkQb7"
   },
   "outputs": [],
   "source": [
    "from nltk import bigrams\n",
    "\n",
    "# Create a placeholder for the model\n",
    "bi_model = defaultdict(lambda: defaultdict(lambda: 0))\n",
    "\n",
    "# Count the frequency of each bigram\n",
    "for sentence in reuters.sents():\n",
    "    for w1, w2 in bigrams(\n",
    "        sentence,\n",
    "        pad_right=True,\n",
    "        pad_left=True,\n",
    "        left_pad_symbol=\"<s>\",\n",
    "        right_pad_symbol=\"</s>\",\n",
    "    ):\n",
    "        bi_model[w1][w2] += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bi_model\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As before, we need to transform counts into probabilities. For that, we divide each count by the total number of occurrences of the first word in the bigram.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n",
    "for w1 in bi_model:\n",
    "    total_count = float(sum(bi_model[w1].values()))\n",
    "    for w2 in bi_model[w1]:\n",
    "        bi_model[w1][w2] /= total_count\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bi_model\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Likely pairs\n",
    "\n",
    "What are the probabilities of each word following 'today'?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1jvD75QekQb9"
   },
   "outputs": [],
   "source": [
    "# your code here\n",
    "print(bi_model[\"today\"])\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What are the probabilities for sentence-starting words? What do most of them have in common? (Hint: check the _left_pad_symbol_ defined above for collecting bigrams.)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1jvD75QekQb9"
   },
   "outputs": [],
   "source": [
    "# your code here\n",
    "print(bi_model[\"<s>\"])\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generating text\n",
    "\n",
    "Now that we have a bigram model, we can generate text based on it.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xRXR0uxHkQb9"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "# sequence start symbol\n",
    "text = [\"<s>\"]\n",
    "\n",
    "# generate text until we find the end of sequence symbol\n",
    "while text[-1] != \"</s>\":\n",
    "    # select a random probability threshold\n",
    "    r = random.random()\n",
    "\n",
    "    # select word above the probability threshold, conditioned to the previous word text[-1]\n",
    "    accumulator = 0.0\n",
    "    for word in bi_model[text[-1]].keys():\n",
    "        accumulator += bi_model[text[-1]][word]\n",
    "        if accumulator >= r:\n",
    "            text.append(word)\n",
    "            break\n",
    "\n",
    "print(\" \".join([t for t in text if t]))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "NAjueaY4kQb-"
   },
   "source": [
    "## Trigram model\n",
    "\n",
    "In a trigram model, we'll compute the probability of each word given the previous two words as context. To obtain trigrams, we can use NLTK's [trigrams](https://www.nltk.org/_modules/nltk/util.html#trigrams).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UFpa4uXHkQb_"
   },
   "outputs": [],
   "source": [
    "# your code here\n",
    "from nltk import trigrams\n",
    "\n",
    "# Create a placeholder for the model\n",
    "tri_model = defaultdict(lambda: defaultdict(lambda: defaultdict(lambda: 0)))\n",
    "\n",
    "# Count the frequency of each trigram\n",
    "for sentence in reuters.sents():\n",
    "    for w1, w2, w3 in trigrams(\n",
    "        sentence,\n",
    "        pad_right=True,\n",
    "        pad_left=True,\n",
    "        left_pad_symbol=\"<s>\",\n",
    "        right_pad_symbol=\"</s>\",\n",
    "    ):\n",
    "        tri_model[w1][w2][w3] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tri_model\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Likely triplets\n",
    "\n",
    "What are the most likely words following \"today the\"?\n",
    "What about \"England has\"?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for w1 in tri_model:\n",
    "    for w2 in tri_model[w1]:\n",
    "        total_count = float(sum(tri_model[w1][w2].values()))\n",
    "        for w3 in tri_model[w1][w2]:\n",
    "            tri_model[w1][w2][w3] /= total_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tri_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tri_model[\"today\"][\"the\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tri_model[\"England\"][\"has\"]\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generating text\n",
    "\n",
    "Create your text generator based on the trigram model. Does the generated text start to feel a bit more sound?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YV2b_wVfkQcB"
   },
   "outputs": [],
   "source": [
    "# your code here\n",
    "import random\n",
    "\n",
    "# sequence start symbol\n",
    "text = [\"<s>\", \"<s>\"]\n",
    "\n",
    "# generate text until we find the end of sequence symbol\n",
    "while text[-1] != \"</s>\":\n",
    "    # select a random probability threshold\n",
    "    r = random.random()\n",
    "\n",
    "    # select word above the probability threshold, conditioned to the previous two words text[-1]\n",
    "    accumulator = 0.0\n",
    "    for word in tri_model[text[-2]][text[-1]].keys():\n",
    "        accumulator += tri_model[text[-2]][text[-1]][word]\n",
    "        if accumulator >= r:\n",
    "            text.append(word)\n",
    "            break\n",
    "\n",
    "print(\" \".join([t for t in text[1:] if t]))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "ngWW-8YXkQcC"
   },
   "source": [
    "## N-gram models\n",
    "\n",
    "For larger _n_, we can use NLTK's [n-grams](https://www.nltk.org/_modules/nltk/util.html#ngrams), which allows us to choose an arbitrary _n_.\n",
    "\n",
    "Create your own 4-gram model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tDB-aucOkQcC"
   },
   "outputs": [],
   "source": [
    "# your code here\n",
    "# your code here\n",
    "from nltk import ngrams\n",
    "\n",
    "# Create a placeholder for the model\n",
    "four_model = defaultdict(lambda: defaultdict(lambda: defaultdict(lambda: defaultdict(lambda: 0))))\n",
    "\n",
    "# Count the frequency of each trigram\n",
    "for sentence in reuters.sents():\n",
    "    for w1, w2, w3, w4 in ngrams(\n",
    "        sentence,\n",
    "        4,\n",
    "        pad_right=True,\n",
    "        pad_left=True,\n",
    "        left_pad_symbol=\"<s>\",\n",
    "        right_pad_symbol=\"</s>\",\n",
    "    ):\n",
    "        four_model[w1][w2][w3][w4] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "four_model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Likely tuples\n",
    "\n",
    "Check the most likely words following \"today the public\".\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HkkHxPTTkQcD"
   },
   "outputs": [],
   "source": [
    "# your code here\n",
    "for w1 in four_model:\n",
    "    for w2 in four_model[w1]:\n",
    "        for w3 in four_model[w1][w2]:\n",
    "            total_count = float(sum(four_model[w1][w2][w3].values()))\n",
    "            for w4 in four_model[w1][w2][w3]:\n",
    "                four_model[w1][w2][w3][w4] /= total_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "four_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "four_model[\"today\"][\"the\"][\"public\"]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generating text\n",
    "\n",
    "Create your text generator based on the 4-gram model. Even better, uh?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OU3XXz10kQcD"
   },
   "outputs": [],
   "source": [
    "# your code here\n",
    "import random\n",
    "\n",
    "# sequence start symbol\n",
    "text = [\"<s>\", \"<s>\", \"<s>\"]\n",
    "\n",
    "# generate text until we find the end of sequence symbol\n",
    "while text[-1] != \"</s>\":\n",
    "    # select a random probability threshold\n",
    "    r = random.random()\n",
    "\n",
    "    # select word above the probability threshold, conditioned to the previous two words text[-1]\n",
    "    accumulator = 0.0\n",
    "    for word in four_model[text[-3]][text[-2]][text[-1]].keys():\n",
    "        accumulator += four_model[text[-3]][text[-2]][text[-1]][word]\n",
    "        if accumulator >= r:\n",
    "            text.append(word)\n",
    "            break\n",
    "\n",
    "print(\" \".join([t for t in text[2:] if t]))\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "language-models.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1 (main, Jan  6 2023, 00:00:00) [GCC 12.2.1 20221121 (Red Hat 12.2.1-4)]"
  },
  "vscode": {
   "interpreter": {
    "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
